{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9395c475",
   "metadata": {},
   "source": [
    "## DSC 180AB Data Science Capstone\n",
    "### Replication Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49e865",
   "metadata": {},
   "source": [
    "Team Members: Wenbin Jiang, AJ Falak, and Rongjing Jiang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1c3d2",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "To return to the table of contents, click on the number at any major section heading."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dacfbb4",
   "metadata": {},
   "source": [
    "[1. Introduction](#1.-Introduction)\n",
    "\n",
    "[2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n",
    "\n",
    "[3. Model Development](#3.-Model-Development)\n",
    "\n",
    "[4. Model Evaluation](#4.-Model-Evaluation)\n",
    "\n",
    "[5. Bias Mitigation](#5.-Bias-Mitigation)\n",
    "\n",
    "[6. Results Summary](#6.-Results-Summary)\n",
    "\n",
    "[7. Explainability](#7.-Explainability)\n",
    "\n",
    "[8. Conclusion & Discussion](#8.-Conclusion-&-Discussion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76354145-7d96-4a89-92ec-4dfa713ff959",
   "metadata": {},
   "source": [
    "## This tutorial demonstrates classification model learning with bias mitigation as a part of a Care Management use case using Medical Expenditure data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799d7d3-7572-4e08-9e3a-812ba39befda",
   "metadata": {},
   "source": [
    "The notebook demonstrates how the AIF 360 toolkit can be used to detect and reduce bias when learning classifiers using a variety of fairness metrics and algorithms. It also demonstrates how explanations can be generated for predictions made by models learnt with the toolkit using LIME.\n",
    "\n",
    "* Classifiers are built using Logistic Regression as well as Random Forests.\n",
    "* Bias detection is demonstrated using several metrics, including disparate impact, average odds difference, statistical parity difference, equal opportunity difference, and Theil index.\n",
    "* Bias alleviation is explored via a variety of methods, including reweighing (pre-processing algorithm), prejudice remover (in-processing algorithm), and disparate impact remover (pre-processing technique).\n",
    "* Data from the [Medical Expenditure Panel Survey](https://meps.ahrq.gov/mepsweb/) is used in this tutorial.\n",
    "\n",
    "\n",
    "The Medical Expenditure Panel Survey (MEPS) provides nationally representative estimates of health expenditure, utilization, payment sources, health status, and health insurance coverage among the noninstitutionalized U.S. population. These government-produced data sets examine how people use the US healthcare system.\n",
    "\n",
    "MEPS is administered by the Agency for Healthcare Research and Quality (AHRQ) and is divided into three components: \n",
    "* Household\n",
    "* Insurance/Employer, and \n",
    "* Medical Provider. \n",
    "\n",
    "These components provide comprehensive national estimates of health care use and payment by individuals, families, and any other demographic group of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad3269",
   "metadata": {},
   "source": [
    "### [1.](#Table-of-Contents) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682f615",
   "metadata": {},
   "source": [
    "The [AI Fairness 360 toolkit](https://github.com/Trusted-AI/AIF360) is an extensible open-source library containing techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle. AI Fairness 360 documentation is available [here](https://aif360.readthedocs.io/en/stable/).\n",
    "\n",
    "The AI Fairness 360 package includes: \n",
    "- a comprehensive set of metrics for datasets and models to test for biases,\n",
    "- explanations for these metrics, and\n",
    "- algorithms to mitigate bias in datasets and models\n",
    "It is designed to translate algorithmic research from the lab into the actual practice of domains as wide-ranging as finance, human capital management, healthcare, and education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523227a6-425b-4dba-bb8a-81ea10e59102",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.1 Use Case\n",
    "\n",
    "**In order to demonstrate how AIF360 can be used to detect and mitigate bias in classifier models, we adopt the following use case:**\n",
    "\n",
    "* Data scientist develops a 'fair' healthcare utilization scoring model with respect to defined protected classes. Fairness may be dictated by legal or government regulations, such as a requirement that additional care decisions be not predicated on factors such as race of the patient.\n",
    "* Developer takes the model AND performance characteristics / specs of the model (e.g. accuracy, fairness tests, etc. basically the model factsheet) and deploys the model in an enterprise app that prioritizes cases for care management.\n",
    "* The app is put into production and starts scoring people and making recommendations. \n",
    "* Explanations are generated for each recommendation\n",
    "* Both recommendations and associated explanations are given to nurses as a part of the care management process. The nurses can evaluate the recommendations for quality and correctness and provide feedback.\n",
    "* Nurse feedback as well as analysis of usage data with respect to specs of the model w.r.t accuracy and fairness is communicated to AI Ops specialist and LOB user periodically.\n",
    "* When significant drift in model specs relative to the model factsheet is observed, the model is sent back for retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b3fccd-dada-42d4-a408-42582b8d060f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1.2 Data\n",
    "Released as an ASCII file (with related SAS, SPSS, and STATA programming statements) and a SAS transport dataset, this public use file provides information collected on a nationally representative sample of the civilian noninstitutionalized population of the United States for calendar year 2015. This file consists of MEPS survey data obtained in Rounds 3, 4, and 5 of Panel 19 and Rounds 1, 2, and 3 of Panel 20 (i.e., the rounds for the MEPS panels covering calendar year 2015) and consolidates all of the final 2015 person-level variables onto one file. This file contains the following variables previously released on HC-174: survey administration, language of interview variable, demographics, parent identifiers, health status, disability days variables, access to care, employment, quality of care, patient satisfaction, health insurance, and use variables. The HC-181 file also includes these variables: income variables and expenditure variables.\n",
    "\n",
    "The specific data used is the [2015 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) as well as the [2016 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192).\n",
    "\n",
    "* The 2015 file contains data from rounds 3,4,5 of panel 19 (2014) and rounds 1,2,3 of panel 20 (2015). \n",
    "* The 2016 file contains data from rounds 3,4,5 of panel 20 (2015) and rounds 1,2,3 of panel 21 (2016).\n",
    "\n",
    "In this example, three datasets were constructed: one from panel 19, round 5 (used for learning models), one from panel 20, round 3 (used for deployment/testing of model - steps); the other from panel 21, round 3 (used for re-training and deployment/testing of updated model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcd0b66-89d3-4e62-a3af-1b62b3dbaf27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 1.3 Methodology \n",
    "\n",
    "For each dataset, the sensitive attribute is 'RACE' constructed as follows: 'Whites' (privileged class) defined by the features RACEV2X = 1 (White) and HISPANX = 2 (non Hispanic); 'Non-Whites' that included everyone else.  \n",
    "\n",
    "* Along with race as the sensitive feature, other features used for modeling include demographics  (such as age, gender, active duty status), physical/mental health assessments, diagnosis codes (such as history of diagnosis of cancer, or diabetes), and limitations (such as cognitive or hearing or vision limitation).\n",
    "* To measure utilization, a composite feature, 'UTILIZATION', was created to measure the total number of trips requiring some sort of medical care by summing up the following features: OBTOTV15(16), the number of office based visits;  OPTOTV15(16), the number of outpatient visits; ERTOT15(16), the number of ER visits;  IPNGTD15(16), the number of inpatient nights, and  + HHTOTD16, the number of home health visits.\n",
    "* The model classification task is to predict whether a person would have 'high' utilization (defined as UTILIZATION >= 10, roughly the average utilization for the considered population). High utilization respondents constituted around 17% of each dataset.\n",
    "* To simulate the scenario, each dataset is split into 3 parts: a train, a validation, and a test/deployment part.\n",
    "\n",
    "**We assume that the model is initially built and tuned using the 2015 Panel 19 train/test data**\n",
    "* It is then put into practice and used to score people to identify potential candidates for care management. \n",
    "* Initial deployment is simulated to 2015 Panel 20 deployment data. \n",
    "* To show change in performance and/or fairness over time, the 2016 Panel 21 deployment data is used. \n",
    "* Finally, if drift is observed, the 2015 train/validation data is used to learn a new model and evaluated again on the 2016 deployment data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a44da",
   "metadata": {},
   "source": [
    "### 1.4 Insert writeup of overall replication project goals and big picture thinking (2-3 paragraphs).  \n",
    "* Why do we care about this? \n",
    "* What would the benefit of predicting utilization be? \n",
    "* What might occur if there are errors?\n",
    "* Who are the affected parties and stakeholders?\n",
    "* Other thoughts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd7569b-9388-4939-9422-6d134dbc0303",
   "metadata": {},
   "source": [
    "**Write up here:**\n",
    "\n",
    "This data gives insights into mental and physical health across different ages. Based on the results, people can better target groups based on their mental/physical health scores. For example, if we find there to be a high correlation between people in a specific age/demographic and their physical health, we can focus efforts on improving health decisions for these groups.\n",
    "This would allow resource allocation to much more efficient. If we were to accurately predict this it could reduce wait times and help with overall efficiency.For example, healthcare utilization prediction, measuring patients requiring 10 or more medical visits, serves as a vital tool for healthcare systems worldwide, enabling providers to optimize resources regardless of their public or private framework. \n",
    "\n",
    "\n",
    "\n",
    "In a dataset like this, errors can have a huge real-world impact, especially in terms of resource utilization. As mentioned earlier, if we conclude that specific groups need more resources/help but find out that there were errors in the data/algorithm, then we would be incorrectly using resources that could be helping demographics that need it more.\n",
    "The affected parties and stakeholders would include anyone from the healthcare sector. This would include patients, healthcare providers, insurance companies, and policymakers. For instance, patients and healthcare providers may depend on the model for identification and health assessments. It is crucial that the predictions it makes are accurate and precise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f776c5-8ed6-49c3-9f12-f2afed0605d4",
   "metadata": {},
   "source": [
    "---\n",
    "End of Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1413d0",
   "metadata": {},
   "source": [
    "### [2.](#Table-of-Contents) Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4a57f-abcf-47c6-961e-baec728e930a",
   "metadata": {},
   "source": [
    "The specific data used is the [2015 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) as well as the [2016 Full Year Consolidated Data File](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-192).\n",
    "\n",
    "* The 2015 file contains data from rounds 3,4,5 of panel 19 (2014) and rounds 1,2,3 of panel 20 (2015). \n",
    "* The 2016 file contains data from rounds 3,4,5 of panel 20 (2015) and rounds 1,2,3 of panel 21 (2016).\n",
    "\n",
    "In this example, three datasets were constructed: one from panel 19, round 5 (used for learning models), one from panel 20, round 3 (used for deployment/testing of model - steps); the other from panel 21, round 3 (used for re-training and deployment/testing of updated model).\n",
    "\n",
    "See the corresponding [Codebook](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) for information on variables.\n",
    "\n",
    "##### Key MEPS dataset features include:\n",
    "* **Utilization**: To measure utilization, a composite feature, 'UTILIZATION', was created to measure the total number of trips requiring some sort of medical care by summing up the following features: OBTOTV15(16), the number of office based visits;  OPTOTV15(16), the number of outpatient visits; ERTOT15(16), the number of ER visits;  IPNGTD15(16), the number of inpatient nights, and  + HHTOTD16, the number of home health visits.\n",
    "* The model classification task is to predict whether a person would have **'high'** utilization (defined as UTILIZATION >= 10, roughly the average utilization for the considered population). High utilization respondents constituted around 17% of each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7368b94-7451-4cf1-8a66-229d2a07907d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.0 Pre-processing Scripts (for each Panel)\n",
    "\n",
    "There is currently minimal EDA for this tutorial within IBM AIF360 Medical Expenditure Tutorial. Therefore, we have adapted  utility scripts from IBM AIF360 Tutorial for ease of understanding for how datasets were pre-processed. These will be used primarily for EDA purposes. We will utilize IBM's tutorial for the remainder of the project. We have utilized Pandas for this portion of the project. \n",
    "\n",
    "**Note:** these pre-processing script below are run for each data file, and then filtered for each panel. This was done in order to match subsequent portions of the tutorial, and how train/test/validation datasets were split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b833fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Get and Load Dataset, Apply Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36982c-27bb-4159-9d99-1730c1ca8b6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Before Proceeding Ensure You Have:**\n",
    "* Forked the AIF360 repository and cloned locally to your disk or virtual machine\n",
    "* Downloaded the `h181.csv` and `h192.csv` data files uploaded [here](https://www.kaggle.com/datasets/nanrahman/mepsdata)\n",
    "* Place the `h181.csv` and `h192.csv` in a folder you can access (we placed it in `../aif360/data/raw/meps/` of our forked AIF360 repository)\n",
    "* For EDA we only focus on `h181.csv` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52b887ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 15:20:38.303029: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/miniconda3/envs/aif360/lib/python3.9/site-packages/torch/_functorch/deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98db0e2c-0708-42c4-a854-3c074c893a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_181 = pd.read_csv('/Users/grace/Desktop/AIF360/aif360/data/raw/meps/h181.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365299c-679b-4022-93fe-de6017aec710",
   "metadata": {},
   "source": [
    "#### Apply pre-processing scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeb4dd-572c-4365-b137-d3b6b4121e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_mappings = {\n",
    "    'label_maps': [{1.0: '>= 10 Visits', 0.0: '< 10 Visits'}],\n",
    "    'protected_attribute_maps': [{1.0: 'White', 0.0: 'Non-White'}]}\n",
    "\n",
    "def default_preprocessing19(df):\n",
    "    \"\"\"\n",
    "    1.Create a new column, RACE that is 'White' if RACEV2X = 1 and HISPANX = 2 i.e. non Hispanic White\n",
    "      and 'non-White' otherwise\n",
    "    2. Restrict to Panel 19\n",
    "    3. RENAME all columns that are PANEL/ROUND SPECIFIC\n",
    "    4. Drop rows based on certain values of individual features that correspond to missing/unknown - generally < -1\n",
    "    5. Compute UTILIZATION, binarize it to 0 (< 10) and 1 (>= 10)\n",
    "    \"\"\"\n",
    "    def race(row):\n",
    "        if ((row['HISPANX'] == 2) and (row['RACEV2X'] == 1)):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE#return 'White'\n",
    "            return 'Non-White'\n",
    "\n",
    "    df['RACEV2X'] = df.apply(lambda row: race(row), axis=1)\n",
    "    df = df.rename(columns = {'RACEV2X' : 'RACE'})\n",
    "\n",
    "    df = df[df['PANEL'] == 19]\n",
    "\n",
    "    # RENAME COLUMNS\n",
    "    df = df.rename(columns = {'FTSTU53X' : 'FTSTU', 'ACTDTY53' : 'ACTDTY', 'HONRDC53' : 'HONRDC', 'RTHLTH53' : 'RTHLTH',\n",
    "                              'MNHLTH53' : 'MNHLTH', 'CHBRON53' : 'CHBRON', 'JTPAIN53' : 'JTPAIN', 'PREGNT53' : 'PREGNT',\n",
    "                              'WLKLIM53' : 'WLKLIM', 'ACTLIM53' : 'ACTLIM', 'SOCLIM53' : 'SOCLIM', 'COGLIM53' : 'COGLIM',\n",
    "                              'EMPST53' : 'EMPST', 'REGION53' : 'REGION', 'MARRY53X' : 'MARRY', 'AGE53X' : 'AGE',\n",
    "                              'POVCAT15' : 'POVCAT', 'INSCOV15' : 'INSCOV'})\n",
    "\n",
    "    df = df[df['REGION'] >= 0] # remove values -1\n",
    "    df = df[df['AGE'] >= 0] # remove values -1\n",
    "\n",
    "    df = df[df['MARRY'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[df['ASTHDX'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[(df[['FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX','EDUCYR','HIDEG',\n",
    "                             'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                             'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','WLKLIM',\n",
    "                             'ACTLIM','SOCLIM','COGLIM','DFHEAR42','DFSEE42','ADSMOK42',\n",
    "                             'PHQ242','EMPST','POVCAT','INSCOV']] >= -1).all(1)]  #for all other categorical features, remove values < -1\n",
    "\n",
    "    def utilization(row):\n",
    "        return row['OBTOTV15'] + row['OPTOTV15'] + row['ERTOT15'] + row['IPNGTD15'] + row['HHTOTD15']\n",
    "\n",
    "    df['TOTEXP15'] = df.apply(lambda row: utilization(row), axis=1)\n",
    "    lessE = df['TOTEXP15'] < 10.0\n",
    "    df.loc[lessE,'TOTEXP15'] = 0.0\n",
    "    moreE = df['TOTEXP15'] >= 10.0\n",
    "    df.loc[moreE,'TOTEXP15'] = 1.0\n",
    "\n",
    "    df = df.rename(columns = {'TOTEXP15' : 'UTILIZATION'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df0a46-3c8d-4359-a955-43aff0a118eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_preprocessing20(df):\n",
    "    \"\"\"\n",
    "    1.Create a new column, RACE that is 'White' if RACEV2X = 1 and HISPANX = 2 i.e. non Hispanic White\n",
    "      and 'non-White' otherwise\n",
    "    2. Restrict to Panel 20\n",
    "    3. RENAME all columns that are PANEL/ROUND SPECIFIC\n",
    "    4. Drop rows based on certain values of individual features that correspond to missing/unknown - generally < -1\n",
    "    5. Compute UTILIZATION, binarize it to 0 (< 10) and 1 (>= 10)\n",
    "    \"\"\"\n",
    "    def race(row):\n",
    "        if ((row['HISPANX'] == 2) and (row['RACEV2X'] == 1)):  #non-Hispanic Whites are marked as WHITE; all others as NON-WHITE\n",
    "            return 'White'\n",
    "        return 'Non-White'\n",
    "\n",
    "    df['RACEV2X'] = df.apply(lambda row: race(row), axis=1)\n",
    "    df = df.rename(columns = {'RACEV2X' : 'RACE'})\n",
    "\n",
    "    df = df[df['PANEL'] == 20]\n",
    "\n",
    "    # RENAME COLUMNS\n",
    "    df = df.rename(columns = {'FTSTU53X' : 'FTSTU', 'ACTDTY53' : 'ACTDTY', 'HONRDC53' : 'HONRDC', 'RTHLTH53' : 'RTHLTH',\n",
    "                              'MNHLTH53' : 'MNHLTH', 'CHBRON53' : 'CHBRON', 'JTPAIN53' : 'JTPAIN', 'PREGNT53' : 'PREGNT',\n",
    "                              'WLKLIM53' : 'WLKLIM', 'ACTLIM53' : 'ACTLIM', 'SOCLIM53' : 'SOCLIM', 'COGLIM53' : 'COGLIM',\n",
    "                              'EMPST53' : 'EMPST', 'REGION53' : 'REGION', 'MARRY53X' : 'MARRY', 'AGE53X' : 'AGE',\n",
    "                              'POVCAT15' : 'POVCAT', 'INSCOV15' : 'INSCOV'})\n",
    "\n",
    "    df = df[df['REGION'] >= 0] # remove values -1\n",
    "    df = df[df['AGE'] >= 0] # remove values -1\n",
    "\n",
    "    df = df[df['MARRY'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[df['ASTHDX'] >= 0] # remove values -1, -7, -8, -9\n",
    "\n",
    "    df = df[(df[['FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX','EDUCYR','HIDEG',\n",
    "                             'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                             'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','WLKLIM',\n",
    "                             'ACTLIM','SOCLIM','COGLIM','DFHEAR42','DFSEE42','ADSMOK42',\n",
    "                             'PHQ242','EMPST','POVCAT','INSCOV']] >= -1).all(1)]  #for all other categorical features, remove values < -1\n",
    "\n",
    "    def utilization(row):\n",
    "        return row['OBTOTV15'] + row['OPTOTV15'] + row['ERTOT15'] + row['IPNGTD15'] + row['HHTOTD15']\n",
    "\n",
    "    df['TOTEXP15'] = df.apply(lambda row: utilization(row), axis=1)\n",
    "    lessE = df['TOTEXP15'] < 10.0\n",
    "    df.loc[lessE,'TOTEXP15'] = 0.0\n",
    "    moreE = df['TOTEXP15'] >= 10.0\n",
    "    df.loc[moreE,'TOTEXP15'] = 1.0\n",
    "\n",
    "    df = df.rename(columns = {'TOTEXP15' : 'UTILIZATION'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c23a2c-9148-466c-8400-64f425d2e53f",
   "metadata": {},
   "source": [
    "#### Taken from pre-processing scripts to retain same columns used in model development for tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c6d234-d213-4bda-96c2-e11605af0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name='UTILIZATION'\n",
    "favorable_classes=[1.0]\n",
    "protected_attribute_names=['RACE']\n",
    "privileged_classes=[['White']]\n",
    "instance_weights_name='PERWT15F'\n",
    "categorical_features=['REGION','SEX','MARRY',\n",
    "                                 'FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX',\n",
    "                                 'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                                 'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','WLKLIM',\n",
    "                                 'ACTLIM','SOCLIM','COGLIM','DFHEAR42','DFSEE42', 'ADSMOK42', 'PHQ242',\n",
    "                                 'EMPST','POVCAT','INSCOV']\n",
    "\n",
    "features_to_keep=['REGION','AGE','SEX','RACE','MARRY',\n",
    "                                 'FTSTU','ACTDTY','HONRDC','RTHLTH','MNHLTH','HIBPDX','CHDDX','ANGIDX',\n",
    "                                 'MIDX','OHRTDX','STRKDX','EMPHDX','CHBRON','CHOLDX','CANCERDX','DIABDX',\n",
    "                                 'JTPAIN','ARTHDX','ARTHTYPE','ASTHDX','ADHDADDX','PREGNT','WLKLIM',\n",
    "                                 'ACTLIM','SOCLIM','COGLIM','DFHEAR42','DFSEE42', 'ADSMOK42',\n",
    "                                 'PCS42','MCS42','K6SUM42','PHQ242','EMPST','POVCAT','INSCOV','UTILIZATION', 'PERWT15F']\n",
    "features_to_drop=[]\n",
    "na_values=[]\n",
    "# custom_preprocessing=default_preprocessing <- don't need this yet for EDA\n",
    "metadata=default_mappings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c2e7d5-b7b6-4af2-a067-25a1102f1e1b",
   "metadata": {},
   "source": [
    "We encourage you to search through the repository and take a look at these scripts, \n",
    "they can be found in `../aif360/dataset/` in your forked AIF360 repository:\n",
    "* AIF360/aif360/datasets/meps_dataset_panel19_fy2015.py\n",
    "* AIF360/aif360/datasets/meps_dataset_panel20_fy2015.py\n",
    "\n",
    "To Explore the `Utilization` and `RACE` features, and the variables used to impute these features:\n",
    "* See the corresponding [HC 181 Codebook](https://meps.ahrq.gov/mepsweb/data_stats/download_data_files_detail.jsp?cboPufNumber=HC-181) for information on variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc7d333-0247-4e92-a550-5f28a6739752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19 = default_preprocessing19(raw_181)\n",
    "df_panel_19_reduced = df_panel_19[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2602e0e-e24e-4362-b378-6a1d2b24baff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_20 = default_preprocessing20(raw_181)\n",
    "df_panel_20_reduced = df_panel_20[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccad607-d831-432b-b182-14dddc959a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### END OF PRE-PROCRESSING ####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e18fec6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2 Data shape and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59741637",
   "metadata": {},
   "source": [
    "First thing is we want to check the dimensions of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16e7d10-db43-4bfb-b8b5-14d79da573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_panel_19_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3ac4e",
   "metadata": {},
   "source": [
    "Also we want to view the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e602c93-d281-4821-8b1c-67125238a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_panel_19_reduced.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202e1ba7",
   "metadata": {},
   "source": [
    "Get an overview of the dataset - top five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9eb4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e64278",
   "metadata": {},
   "source": [
    "Get an overview of the dataset - bottom five"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ad3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19_reduced.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32e32d",
   "metadata": {},
   "source": [
    "We can view the summary statistics of numerical columns with df.describe() method. It enable us to detect outliers in the data which require further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19_reduced.dtypes\n",
    "\n",
    "df_panel_19_reduced.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81462c5",
   "metadata": {},
   "source": [
    "**Looking at this data, here are the key conclusions we can draw:**\n",
    "\n",
    "1. Demographics:\n",
    "    - The average age is around 35 years, with a wide spread (std dev ≈ 22.5 years), suggesting a diverse age range\n",
    "    - There's a slightly higher proportion of one gender over another (mean of 1.52 for SEX, suggesting roughly 52% are coded as \"2\")\n",
    "    - People are spread across 4 regions, with a slight skew toward certain regions (mean 2.83)\n",
    "\n",
    "\n",
    "2. Marital Status:\n",
    "    - The MARRY variable shows significant variation (mean 3.66, std dev 2.12)\n",
    "    - The range is 1-10, suggesting multiple categories for relationship status\n",
    "    - Most people fall in the lower categories (median = 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b934d54d",
   "metadata": {},
   "source": [
    "The marital status is kinda to grab our eyes so I want to dig more into this sepcify category.\n",
    "\n",
    "Here I have the data preparation step in analysis pipelines, making the data more structured and easier to analyze for age-related trends and patterns.\n",
    "\n",
    "First, it replaces all -1 values with NaN (Not a Number) to properly handle missing data, making it easier for statistical analysis and data manipulation. Second, it creates a new column 'AGE_GROUP' that categorizes ages into five meaningful groups (0-18, 19-35, 36-50, 51-65, 65+), which transforms the continuous age variable into categorical groups for better demographic analysis and trend identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e59f165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -1 with NaN as it appears to be a missing value indicator\n",
    "df_panel_19_reduced = df_panel_19_reduced.replace(-1, np.nan)\n",
    "    \n",
    "# Create age groups for better analysis\n",
    "df_panel_19_reduced['AGE_GROUP'] = pd.cut(df_panel_19_reduced['AGE'], \n",
    "                            bins=[0, 18, 35, 50, 65, 100],\n",
    "                            labels=['0-18', '19-35', '36-50', '51-65', '65+'])\n",
    "\n",
    "df_panel_19_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbd1f1",
   "metadata": {},
   "source": [
    "Through these four graphs, we can see some interesting patterns in our population. \n",
    "\n",
    "The age distribution shows that we have participants across all age groups, with a larger concentration of people between 20-60 years old, and fewer participants in the very young and elderly categories. In terms of gender balance, the population is almost evenly split, with slightly more females (52.1%) than males (47.9%).\n",
    "\n",
    "\n",
    "The race data shows that all participants in this survey were classified as 'Non-White', which suggests this might be a focused study on a specific demographic group. This could also mean that Non-White was the only Non-Null race that was inputted. We will explore this further below. \n",
    "\n",
    "As for marriage status, we can see that categories 1, 5, and 6 are the most common in our dataset. Specifically, category 1 typically represents 'Married' individuals, category 5 refers to those who are 'Widowed', and category 6 denotes respondents under the age of 16, for whom marital status is deemed 'Inapplicable'. These categories help us understand the various relationship statuses within the surveyed population. \n",
    "\n",
    "These demographics suggest we're looking at a diverse adult population in terms of age and gender, but with specific racial characteristics, which should be considered when interpreting any broader findings from this survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb43c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_demographic_distributions(df):\n",
    "    \"\"\"\n",
    "    Create demographic distribution plots\n",
    "    \"\"\"\n",
    "    # Create a figure with 2x2 subplots, adjusting figsize for better layout\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    \n",
    "    # Age distribution\n",
    "    sns.histplot(data=df, x='AGE', bins=30, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('Age Distribution')\n",
    "    \n",
    "    # Race distribution\n",
    "    df['RACE'].value_counts().plot(kind='bar', ax=axs[0, 1])\n",
    "    axs[0, 1].set_title('Race Distribution')\n",
    "    axs[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sex distribution\n",
    "    df['SEX'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axs[1, 0])\n",
    "    axs[1, 0].set_title('Sex Distribution')\n",
    "    \n",
    "    # Marriage status\n",
    "    df['MARRY'].value_counts().plot(kind='bar', ax=axs[1, 1])\n",
    "    axs[1, 1].set_title('Marriage Status Distribution')\n",
    "    axs[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()  # Adjust spacing between subplots\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close(fig)  # Close the figure after displaying\n",
    "\n",
    "plot_demographic_distributions(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55329591-7a00-4b97-8422-29bb20c55966",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3 Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d91e0",
   "metadata": {},
   "source": [
    "**Next step we need to check is there any outlier and how do we handlie it.**\n",
    "\n",
    "**Why outliers matter?**\n",
    "- They can skew our analysis and lead to incorrect conclusions\n",
    "- Some outliers might be data entry errors that need correction\n",
    "- Others might be legitimate but rare cases we need to understand better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c07435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_demographic_outliers(df, fig_num=1):\n",
    "    \"\"\"\n",
    "    Analyze outliers specifically for demographic variables: age, race, sex, and marriage\n",
    "    \"\"\"\n",
    "    # Select only demographic columns and handle NaN values\n",
    "    demographic_cols = ['AGE', 'RACE', 'SEX', 'MARRY']\n",
    "    demo_data = df[demographic_cols].copy()\n",
    "    \n",
    "    # Fill NaN values with appropriate values for each column\n",
    "    demo_data['AGE'] = demo_data['AGE'].fillna(demo_data['AGE'].median())\n",
    "    demo_data['RACE'] = demo_data['RACE'].fillna('Unknown')\n",
    "    demo_data['SEX'] = demo_data['SEX'].fillna(demo_data['SEX'].mode()[0])\n",
    "    demo_data['MARRY'] = demo_data['MARRY'].fillna(demo_data['MARRY'].mode()[0])\n",
    "    \n",
    "    # Create figure for analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    fig.supxlabel(f'Figure {fig_num}: Demographic Outlier Analysis', fontsize=16, y=-0.05)  # Add figure title at the bottom\n",
    "    \n",
    "    # Box plots for numerical variables only (AGE and MARRY)\n",
    "    sns.boxplot(y=demo_data['AGE'], ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Age Distribution')\n",
    "    \n",
    "    sns.boxplot(y=demo_data['MARRY'], ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Marriage Status Distribution')\n",
    "    \n",
    "    # Bar plots for categorical variables (SEX and RACE)\n",
    "    demo_data['SEX'].value_counts().plot(kind='bar', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Sex Distribution')\n",
    "    \n",
    "    demo_data['RACE'].value_counts().plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Race Distribution')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Calculate outliers for numerical variables only\n",
    "    outliers_summary = {}\n",
    "    for col in ['AGE', 'MARRY']:  # Only analyze numerical columns\n",
    "        Q1 = demo_data[col].quantile(0.25)\n",
    "        Q3 = demo_data[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = demo_data[(demo_data[col] < lower_bound) | (demo_data[col] > upper_bound)][col]\n",
    "        outliers_summary[col] = {\n",
    "            'count': len(outliers),\n",
    "            'percentage': (len(outliers) / len(demo_data)) * 100,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'min': demo_data[col].min(),\n",
    "            'max': demo_data[col].max()\n",
    "        }\n",
    "    \n",
    "    return outliers_summary, demo_data\n",
    "\n",
    "# Run the analysis and assign the figure number\n",
    "outliers_summary, demo_data = analyze_demographic_outliers(df_panel_19_reduced, fig_num=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648149d",
   "metadata": {},
   "source": [
    "**Understanding Our Community Demographics (figure 1): Key Findings**\n",
    "\n",
    "1. Age Patterns\n",
    "- Most people in our community are middle-age adults\n",
    "- The typical age is around 30-35 years old\n",
    "- We have a good mix of younger and older adults, ranging mainly from early 20s to mid-50s\n",
    "- This could also be a reason why majority of the marriage status is married, widowed, and inapplicable. \n",
    "\n",
    "**This suggests we're looking at a primarily working-age population**\n",
    "\n",
    "\n",
    "2. Gender Balance\n",
    "- Our community shows a fairly even split between men (1) and women (2)\n",
    "- There's a slightly higher number of one gender, but the difference is small\n",
    "\n",
    "**This balanced representation helps us understand both perspectives fairly.**\n",
    "\n",
    "\n",
    "3. Relationship Status\n",
    "- We see a diverse mix of relationship statuses in our community\n",
    "- This variety reflects different life stages and personal choices\n",
    "- \n",
    "\n",
    "**The distribution suggests we're capturing a good range of family situations.**\n",
    "\n",
    "4. Cultural Diversity\n",
    "- About 5,500-6,000 individuals are identified as Non-White\n",
    "- We note that most demographic information is \"Unknown,\" so right now, its hard to come up with a conclusion about the diversity of the community in right now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035b19d",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------\n",
    "Next step: Comparing Before and After Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be1916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers and add figure labels to comparison plots (Age & Marriage stastus)\n",
    "def plot_comparison_before_after(demo_data, df_cleaned, fig_num=2):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    fig.supxlabel(f'Figure {fig_num}: Comparison of Distributions Before and After Cleaning', fontsize=14, y=-0.05)\n",
    "    \n",
    "    # Before cleaning\n",
    "    sns.boxplot(y=demo_data['AGE'], ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Age Before Cleaning')\n",
    "    \n",
    "    sns.boxplot(y=demo_data['MARRY'], ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Marriage Status Before Cleaning')\n",
    "    \n",
    "    # After cleaning\n",
    "    sns.boxplot(y=df_cleaned['AGE'], ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Age After Cleaning')\n",
    "    \n",
    "    sns.boxplot(y=df_cleaned['MARRY'], ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Marriage Status After Cleaning')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)  # Adjust layout to make space for the suptitle\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Run the comparison plot\n",
    "plot_comparison_before_after(demo_data, df_cleaned, fig_num=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1a53f",
   "metadata": {},
   "source": [
    "Draw the conclustion (Age & Marriage Status):\n",
    "- The overall shape of the distribution remains very similar after outlier removal\n",
    "- This suggests that the outliers weren't severely impacting the central tendency of your data\n",
    "- The whiskers (vertical lines extending from the box) appear slightly more symmetrical in the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers and add figure labels to comparison plots (Sex & Race)\n",
    "def plot_comparison_before_after(demo_data, df_cleaned, fig_num=2):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
    "    fig.supxlabel(f'Figure {fig_num}: Comparison of Distributions Before and After Cleaning', fontsize=14, y=-0.05)\n",
    "    \n",
    "    # Before cleaning\n",
    "    demo_data['SEX'].value_counts().plot(kind='bar', ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Sex Before Cleaning')\n",
    "    \n",
    "    demo_data['RACE'].value_counts().plot(kind='bar', ax=axes[0, 1])\n",
    "    axes[0, 1].set_title('Race Before Cleaning')\n",
    "    \n",
    "    # After cleaning\n",
    "    df_cleaned['SEX'].value_counts().plot(kind='bar', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('Sex After Cleaning')\n",
    "    \n",
    "    df_cleaned['RACE'].value_counts().plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Race After Cleaning')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.15)  # Adjust layout to make space for the figure label\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "# Run the comparison plot\n",
    "plot_comparison_before_after(demo_data, df_cleaned, fig_num=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f2269",
   "metadata": {},
   "source": [
    "Data Reliability:\n",
    "- Our careful data cleaning process shows that our original demographic information was already quite reliable. The patterns we see in both gender and race stayed almost the same before and after cleaning, which gives us confidence in our initial data collection.\n",
    "\n",
    "Gender Balance:\n",
    "- We maintain a good balance between our gender categories, with only slight differences between the groups. This balanced representation remained stable through our cleaning process.\n",
    "\n",
    "Racial Demographics:\n",
    "- While we still have a large number of 'Unknown' entries in our race category, this appears to be a genuine characteristic of our data rather than a data quality issue. The consistent numbers before and after cleaning suggest this is a true reflection of our available information.\n",
    "\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685cce03",
   "metadata": {},
   "source": [
    "Next step\n",
    "- Consider improving initial data collection for race demographics to reduce 'Unknown' entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c97347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def analyze_unknown_race(df):\n",
    "    \"\"\"\n",
    "    Analyze and handle Unknown race entries in the dataset\n",
    "    \"\"\"\n",
    "    # Calculate initial statistics\n",
    "    total_records = len(df)\n",
    "    unknown_count = len(df[df['RACE'] == 'Unknown'])\n",
    "    unknown_percentage = (unknown_count / total_records) * 100\n",
    "    \n",
    "    print(f\"Initial Analysis:\")\n",
    "    print(f\"Total records: {total_records:,}\")\n",
    "    print(f\"Unknown race entries: {unknown_count:,} ({unknown_percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualize Unknown vs Non-Unknown distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot 1: Current Race Distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['RACE'].value_counts().plot(kind='bar')\n",
    "    plt.title('Current Race Distribution')\n",
    "    plt.xlabel('Race Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot 2: Unknown vs Known Proportion\n",
    "    plt.subplot(1, 2, 2)\n",
    "    labels = ['Known', 'Unknown']\n",
    "    sizes = [(total_records - unknown_count), unknown_count]\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title('Proportion of Unknown Race Entries')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return total_records, unknown_count, unknown_percentage\n",
    "\n",
    "def suggest_race_improvements(df):\n",
    "    \"\"\"\n",
    "    Generate specific suggestions for reducing Unknown race entries\n",
    "    \"\"\"\n",
    "    suggestions = {\n",
    "        \"Data Collection Improvements\": [\n",
    "            \"1. Update data collection forms to include clear race categories\",\n",
    "            \"2. Make race field mandatory during data entry\",\n",
    "            \"3. Add 'Prefer not to say' option instead of leaving blank\",\n",
    "            \"4. Implement dropdown menu instead of free-text entry\"\n",
    "        ],\n",
    "        \"Data Quality Checks\": [\n",
    "            \"1. Regular audits of new entries\",\n",
    "            \"2. Follow-up process for Unknown entries\",\n",
    "            \"3. Staff training on proper data collection\",\n",
    "            \"4. Monthly data quality reports\"\n",
    "        ],\n",
    "        \"Proposed Race Categories\": [\n",
    "            \"White\",\n",
    "            \"Black or African American\",\n",
    "            \"Asian\",\n",
    "            \"Hispanic or Latino\",\n",
    "            \"Native American\",\n",
    "            \"Pacific Islander\",\n",
    "            \"Multiple Races\",\n",
    "            \"Other\",\n",
    "            \"Prefer not to say\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return suggestions\n",
    "\n",
    "def implement_race_validation(df):\n",
    "    \"\"\"\n",
    "    Implement basic validation and cleaning for race field\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Define standard race categories\n",
    "    standard_categories = {\n",
    "        'unknown': 'Unknown',\n",
    "        'non-white': 'Non-White',\n",
    "        'white': 'White',\n",
    "        'black': 'Non-White',\n",
    "        'asian': 'Non-White',\n",
    "        'hispanic': 'Non-White',\n",
    "        'latino': 'Non-White',\n",
    "        'native': 'Non-White',\n",
    "        'pacific': 'Non-White'\n",
    "    }\n",
    "    \n",
    "    # Clean existing race entries\n",
    "    df_cleaned['RACE'] = df_cleaned['RACE'].str.lower()\n",
    "    df_cleaned['RACE'] = df_cleaned['RACE'].map(standard_categories).fillna('Unknown')\n",
    "    \n",
    "    # Compare before and after\n",
    "    print(\"\\nData Cleaning Results:\")\n",
    "    print(\"Before cleaning:\")\n",
    "    print(df['RACE'].value_counts())\n",
    "    print(\"\\nAfter cleaning:\")\n",
    "    print(df_cleaned['RACE'].value_counts())\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Run the analysis\n",
    "total_records, unknown_count, unknown_percentage = analyze_unknown_race(df_panel_19_reduced)\n",
    "\n",
    "# Get improvement suggestions\n",
    "suggestions = suggest_race_improvements(df_panel_19_reduced)\n",
    "\n",
    "# Print suggestions\n",
    "print(\"\\nRecommended Improvements:\")\n",
    "for category, items in suggestions.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "# Clean the data\n",
    "df_race_cleaned = implement_race_validation(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275eddf",
   "metadata": {},
   "source": [
    "Since 'RACE' is categorical data so we need to use One-hot encoding that ensures models interpret it correctly without assuming order or hierarchy among categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bba75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the 'RACE' column\n",
    "df_race_encoded = pd.get_dummies(df_race_cleaned, columns=['RACE'], prefix='RACE')\n",
    "\n",
    "# Convert only the one-hot encoded columns to integers\n",
    "race_columns = [col for col in df_race_encoded.columns if col.startswith('RACE_')]\n",
    "df_race_encoded[race_columns] = df_race_encoded[race_columns].astype(int)\n",
    "\n",
    "# Display the first few rows to check the result\n",
    "print(df_race_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebab68a",
   "metadata": {},
   "source": [
    "Now let's talk ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59d1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07e6680",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_race_cleaned['UTILIZATION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031329f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965716c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d60165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9c4b75c-3f0b-4383-9607-758d52283321",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.4 Correlation Analysis\n",
    "---------------------------------\n",
    "\n",
    "!!! BELOW NEED TO RE-CHECK AND MAKE IT MORE EXPLAINABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69640659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(df):\n",
    "    \"\"\"\n",
    "    Analyze correlations between specific demographic variables:\n",
    "    AGE, SEX, RACE, MARRY with UTILIZATION\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe with only the variables we want\n",
    "    selected_cols = ['AGE', 'SEX', 'RACE', 'MARRY', 'UTILIZATION']\n",
    "    df_selected = df[selected_cols].copy()\n",
    "    \n",
    "    # Convert categorical variables to numeric if they aren't already\n",
    "    # Create dummy variables for categorical columns\n",
    "    df_encoded = pd.get_dummies(df_selected, columns=['RACE', 'MARRY'])\n",
    "    \n",
    "    # Keep SEX as is since it's likely already numeric (1/2)\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df_encoded.corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                fmt='.2f',\n",
    "                square=True)\n",
    "    plt.title('Correlation Matrix of Demographic Variables with Utilization')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Return both the plot and the correlation matrix\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "correlation_analysis(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ee258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_marry_categories(df):\n",
    "    \"\"\"\n",
    "    Analyze MARRY categories distribution and relationships\n",
    "    \"\"\"\n",
    "    # Get value counts of original MARRY column\n",
    "    marry_dist = df['MARRY'].value_counts().sort_index()\n",
    "    \n",
    "    print(\"MARRY Categories Distribution:\")\n",
    "    print(\"-----------------------------\")\n",
    "    for category, count in marry_dist.items():\n",
    "        percentage = (count/len(df))*100\n",
    "        print(f\"MARRY_{category}: {count} records ({percentage:.1f}%)\")\n",
    "        \n",
    "    # Look at average utilization by MARRY category\n",
    "    avg_util = df.groupby('MARRY')['UTILIZATION'].agg(['mean', 'count'])\n",
    "    print(\"\\nAverage Utilization by MARRY Category:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(avg_util)\n",
    "\n",
    "analyze_marry_categories(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_marry_analysis(df):\n",
    "    \"\"\"\n",
    "    Create simplified correlation analysis focusing on key variables and categories\n",
    "    \"\"\"\n",
    "    # Create copy of dataframe\n",
    "    df_analysis = df.copy()\n",
    "    \n",
    "    # Create main category groupings\n",
    "    df_analysis['MARRY_STATUS'] = df_analysis['MARRY'].map({\n",
    "        1: 'Married',\n",
    "        2: 'Widowed',\n",
    "        3: 'Divorced',\n",
    "        4: 'Separated',\n",
    "        5: 'Never Married',\n",
    "        6: 'Under 16',\n",
    "        -1: 'Unknown'\n",
    "    })\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Average Utilization by Marriage Status\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.boxplot(data=df_analysis, x='MARRY_STATUS', y='UTILIZATION')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Utilization Distribution by Marriage Status')\n",
    "    \n",
    "    # Plot 2: Age Distribution by Marriage Status\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.boxplot(data=df_analysis, x='MARRY_STATUS', y='AGE')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Age Distribution by Marriage Status')\n",
    "    \n",
    "    # Plot 3: Utilization by Age and Marriage Status\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for status in df_analysis['MARRY_STATUS'].unique():\n",
    "        subset = df_analysis[df_analysis['MARRY_STATUS'] == status]\n",
    "        plt.scatter(subset['AGE'], subset['UTILIZATION'], \n",
    "                   alpha=0.5, label=status)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.title('Utilization vs Age by Marriage Status')\n",
    "    \n",
    "    # Plot 4: Count Distribution\n",
    "    plt.subplot(2, 2, 4)\n",
    "    df_analysis['MARRY_STATUS'].value_counts().plot(kind='bar')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Distribution of Marriage Status')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return df_analysis\n",
    "\n",
    "simplified_marry_analysis(df_panel_19_reduced)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd69b90",
   "metadata": {},
   "source": [
    "For the age distribution, we can see that 'Married' individuals typically span a wide middle age range, 'Widowed' predominantly includes older ages, and 'Under 16' are, as expected, young. An interesting highlight is the number of outliers in the 'Never Married' and 'Widowed' columns. This most likely represent indviduals who never found a partner or got divorced early. \n",
    "\n",
    "The bar chart shows that 'Married' and 'Never Married' are the most common statuses in the dataset, with 'Separated' being the least common.\n",
    "\n",
    "Wehn looking at utilization, it seeems that divorced and widowed are the only ones with utilization. Could this be an error with the data? Let's find out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bed0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_panel_19_reduced[['MARRY', 'UTILIZATION']].groupby('MARRY').mean().sort_values(by='UTILIZATION', ascending= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7914154d",
   "metadata": {},
   "source": [
    "From these results, we can see that widowed (2) has highest utilization rate at 0.483129, which could reflect the older average age and possibly greater healthcare needs. It could also represent be because of additional stress from losing a partner. \n",
    "\n",
    "Similarly, divorced (3) has the second highest, which could potentially due to the stresses and health impacts that may accompany divorce or because of the older average age. \n",
    "\n",
    "Another interesting point is people under 16 (6) Very low utilization at 0.056841, consistent with generally healthier young people who need fewer healthcare services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf3935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = df_panel_19_reduced.select_dtypes(include=['number'])  # This includes int and float types\n",
    "\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "# Focus on 'UTILIZATION' correlations with other variables\n",
    "utilization_correlations = correlation_matrix['UTILIZATION'].sort_values(ascending=False)\n",
    "\n",
    "# Print the correlation values to see the results\n",
    "print(utilization_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01decf0e",
   "metadata": {},
   "source": [
    "This is a lot to digest, so let's look at the most significant ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aba3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(utilization_correlations[(utilization_correlations > 0.2) | (utilization_correlations < -0.2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f1502",
   "metadata": {},
   "source": [
    "#### Positive Correlations\n",
    "AGE: 0.337893,indicating that utilization increases with age, which is expected as older individuals typically require more healthcare services.\n",
    "\n",
    "\n",
    "RTHLTH: 0.291040, suggesting that individuals with a poorer perception of their health tend to use more healthcare services.\n",
    "\n",
    "EMPST: 0.236724, which might indicate that employment status is moderately associated with healthcare utilization.\n",
    "\n",
    "\n",
    "MNHlTH: 0.213671, also points to mental health status having a moderate impact on healthcare service use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152571c6",
   "metadata": {},
   "source": [
    "#### Negative Correlations\n",
    "\n",
    "DIABDX (Diabetes diagnosis): -0.200413, and several other diagnostic variables such as CHDDX (Coronary heart disease), ADHADDX (ADHD), COGLIM (Cognitive limitations), CANCERDX (Cancer), HIBPDX (High blood pressure), which all show moderate negative correlations with UTILIZATION ranging between -0.200413 to -0.255758. These correlations suggest that patients with these conditions might either be underrepresented in the dataset, or they have less frequent need for some types of healthcare services tracked by this metric.\n",
    "\n",
    "ARTHDX (Arthritis): -0.348850, indicating significantly lower utilization, which may need further investigation to understand why this group shows lower utilization rates.\n",
    "\n",
    "ACTLIM, WLKLIM, PCS42: These variables related to activity limitations and physical component summary scores also show strong negative correlations (-0.355324, -0.371022, and -0.379500, respectively), suggesting that individuals with these conditions or limitations have lower utilization rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6081a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e4cb401-c3fa-4916-b71b-5e72c03227c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.5 Other analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c818674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def statistical_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform statistical tests on marriage status relationships\n",
    "    \"\"\"\n",
    "    # Chi-square test of independence\n",
    "    contingency_table = pd.crosstab(df['MARRY'], df['UTILIZATION'] > df['UTILIZATION'].median())\n",
    "    chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
    "    \n",
    "    # ANOVA test for utilization differences between marriage groups\n",
    "    marry_groups = [group['UTILIZATION'].values for name, group in df.groupby('MARRY')]\n",
    "    f_stat, anova_p = stats.f_oneway(*marry_groups)\n",
    "    \n",
    "    results = {\n",
    "        'chi2_statistic': chi2,\n",
    "        'chi2_p_value': p_value,\n",
    "        'f_statistic': f_stat,\n",
    "        'anova_p_value': anova_p\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "statistical_analysis(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37cb445-33ef-4b4e-8079-e7d46e6fbb72",
   "metadata": {},
   "source": [
    "-----\n",
    "End of Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ed0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marry_utilization_patterns(df):\n",
    "    \"\"\"\n",
    "    Analyze patterns between marriage status and utilization\n",
    "    \"\"\"\n",
    "    # Calculate summary statistics\n",
    "    summary = df.groupby('MARRY').agg({\n",
    "        'UTILIZATION': ['mean', 'median', 'std', 'count'],\n",
    "        'AGE': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Calculate utilization rates by age group and marriage status\n",
    "    df['AGE_GROUP'] = pd.cut(df['AGE'], \n",
    "                            bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "                            labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "    \n",
    "    utilization_by_age_marry = df.pivot_table(\n",
    "        values='UTILIZATION',\n",
    "        index='MARRY',\n",
    "        columns='AGE_GROUP',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    return summary, utilization_by_age_marry\n",
    "\n",
    "marry_utilization_patterns(df_panel_19_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90379749-ae30-4908-b400-400254239ecd",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3604122-a08a-4cf2-b1a7-66e678666909",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5e270-5781-4e88-a180-4de7660a1c92",
   "metadata": {},
   "source": [
    "_Items below will be updated as course progress_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da83c738",
   "metadata": {},
   "source": [
    "### [3.](#Table-of-Contents) Model Development without Debiasing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065cc8e5",
   "metadata": {},
   "source": [
    "First, load all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5070d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Datasets\n",
    "from aif360.datasets import MEPSDataset19\n",
    "from aif360.datasets import MEPSDataset20\n",
    "from aif360.datasets import MEPSDataset21\n",
    "\n",
    "# Fairness metrics\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "\n",
    "# Explainers\n",
    "from aif360.explainers import MetricTextExplainer\n",
    "\n",
    "# Scalers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Bias mitigation techniques\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import PrejudiceRemover\n",
    "\n",
    "# LIME\n",
    "from aif360.datasets.lime_encoder import LimeEncoder\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049adaf9",
   "metadata": {},
   "source": [
    "### 3.1. Load data & create splits for learning/validating/testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde781a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset_orig_panel19_train,\n",
    " dataset_orig_panel19_val,\n",
    " dataset_orig_panel19_test) = df_panel_19_reduced().split([0.5, 0.8], shuffle=True)\n",
    "\n",
    "sens_ind = 0\n",
    "sens_attr = dataset_orig_panel19_train.protected_attribute_names[sens_ind]\n",
    "\n",
    "unprivileged_groups = [{sens_attr: v} for v in\n",
    "                       dataset_orig_panel19_train.unprivileged_protected_attributes[sens_ind]]\n",
    "privileged_groups = [{sens_attr: v} for v in\n",
    "                     dataset_orig_panel19_train.privileged_protected_attributes[sens_ind]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e9ff7",
   "metadata": {},
   "source": [
    "### 3.2. Learning a Logistic Regression (LR) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93548ae9",
   "metadata": {},
   "source": [
    "### 3.3. Learning a Random Forest (RF) classifier on original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b69aab",
   "metadata": {},
   "source": [
    "### Section 3 Write Up here\n",
    "\n",
    "### Part-01: For **both** the logistic regression and random forest classifiers learned on the original data, please include explain the results of your fairness metrics. For _each_ metric result briefly describe what this value means in 1-2 sentences (is it fair, is it not fair? Why?)\n",
    "\n",
    "**Fairness Metric Summary** \n",
    "* Threshold corresponding to Best balanced accuracy:\n",
    "* Best balanced accuracy: \n",
    "* Corresponding 1-min(DI, 1/DI) value: \n",
    "* Corresponding average odds difference value: \n",
    "* Corresponding statistical parity difference value: \n",
    "* Corresponding equal opportunity difference value:\n",
    "* Corresponding Theil index value:\n",
    "\n",
    "### Part-02: Please write one paragraph for each question.\n",
    "1. How can we determine which metrics to use, given our data and use case? You can refer to [Course material](https://nanrahman.github.io/capstone-responsible-ai/weeks/06-Fairness-Assessments/), online research and Guidance provided by [AIF360](http://aif360.mybluemix.net/resources#)\n",
    "2. When you have competing fairness metrics, how to pick which to prioritize?\n",
    "3. What do you do when you encounter different definitions for similar metrics?\n",
    "4. Based on this, which model and fairness metric appears the best to proceed with?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d0791",
   "metadata": {},
   "source": [
    "### [4.](#Table-of-Contents) Additional Model Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b500c8",
   "metadata": {},
   "source": [
    "### 4.1A Load data & create splits for learning/validating/testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41619a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same methods from Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa712dc",
   "metadata": {},
   "source": [
    "### 4.3. Learning a Random Forest (RF) classifier on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2fa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same methods from Section 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf4638",
   "metadata": {},
   "source": [
    "### Section 4 Write Up here\n",
    "\n",
    "**1. For both the logistic regression and random forest classifiers learned on the original data, please include the results of your fairness metrics. For _each_ metric result briefly describe (1-2 sentences) if you saw any differences from your results in Part 3, and what that might mean.**\n",
    "\n",
    "_Fairness Metrics_\n",
    "   * Threshold corresponding to Best balanced accuracy:\n",
    "   * Best balanced accuracy: \n",
    "   * Corresponding 1-min(DI, 1/DI) value: \n",
    "   * Corresponding average odds difference value: \n",
    "   * Corresponding statistical parity difference value: \n",
    "   * Corresponding equal opportunity difference value:\n",
    "   * Corresponding Theil index value:\n",
    "    \n",
    "**2. Based on this, would you make any recommendations during model development? Does it change which model and fairness metric would be the best to proceed with?** (Please write at least one paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f1e6bb",
   "metadata": {},
   "source": [
    "\n",
    "### End of Replication Part 02 -  Model Development and Fairness Evaluation\n",
    "\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b3e294",
   "metadata": {},
   "source": [
    "### [5.](#Table-of-Contents) Bias Mitigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d7b57",
   "metadata": {},
   "source": [
    "### [6.](#Table-of-Contents) Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61997b93",
   "metadata": {},
   "source": [
    "### [7.](#Table-of-Contents) Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9010ecc",
   "metadata": {},
   "source": [
    "### [8.](#Table-of-Contents) Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e701db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aif360",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
